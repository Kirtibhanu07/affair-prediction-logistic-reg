# -*- coding: utf-8 -*-
"""Predict Extra Marital Affair Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dDKw82yP5L0HDHuGwUK_BuF_M_6VprXb
"""

#@title Extramarital affairs dataset that comes with Statsmodels used to explain the allocation of an individualâ€™s time among work, time spent with a spouse, and time spent with a paramour. The data is used as an example of regression with censored data.

import numpy as np
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

import statsmodels.api as sm
import matplotlib.pyplot as plt
from patsy import dmatrices
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split 
from sklearn import metrics

data = sm.datasets.fair.load_pandas().data

data.head()

data.shape

data.columns

data.info()

"""The dataset contains 6366 observations of 9 variables:


*   **rate_marriage**: woman's rating of her marriage (1 = very poor, 5 = very good)
*   **age**: woman's age
*   **yrs_married**: number of years married
*   **children**: number of children
*   **religious**: woman's rating of how religious she is (1 = not religious, 4 = strongly religious)
*   **educ**: level of education (9 = grade school, 12 = high school, 14 = some 
*   **college**, 16 = college graduate, 17 = some graduate school, 20 = advanced degree)
*   **occupation**: woman's occupation (1 = student, 2 = farming/semi-skilled/unskilled, 3 = "white collar", 4 = teacher/nurse/writer/technician/skilled, 5 = managerial/business)
*   **managerial/business**, 6 = professional with advanced degree)
*   **occupation_husb**: husband's occupation (same coding as above)
*   **affairs**: time spent in extra-marital affairs













"""

data['affair'] = (data.affairs > 0).astype(int)

data.sample(3)

data.groupby('affair').mean()



# histogram of education
data.rate_marriage.hist()
plt.title('Histogram of Affair')
plt.xlabel('Education Level')
plt.ylabel('Frequency')

# histogram of education
data.educ.hist()
plt.title('Histogram of Education')
plt.xlabel('Education Level')
plt.ylabel('Frequency')

# barplot of marriage rating grouped by affair (True or False)
pd.crosstab(data.rate_marriage, data.affair.astype(bool)).plot(kind='bar')
plt.title('Marriage Rating Distribution by Affair Status')
plt.xlabel('Marriage Rating')
plt.ylabel('Frequency')

affair_yrs_married = pd.crosstab(data.yrs_married, data.affair.astype(bool))
affair_yrs_married.div(affair_yrs_married.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Affair Percentage by Years Married')
plt.xlabel('Years Married')
plt.ylabel('Percentage')

affair_yrs_married = pd.crosstab(data.age, data.affair.astype(bool))
affair_yrs_married.div(affair_yrs_married.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Affair Percentage by Age')
plt.xlabel('Age')
plt.ylabel('Percentage')

affair_yrs_married = pd.crosstab(data.occupation, data.affair.astype(bool))
affair_yrs_married.div(affair_yrs_married.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Affair Percentage by Occupation')
plt.xlabel('Occupation')
plt.ylabel('Percentage')

affair_yrs_married = pd.crosstab(data.educ, data.affair.astype(bool))
affair_yrs_married.div(affair_yrs_married.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Affair Percentage by Education')
plt.xlabel('Education')
plt.ylabel('Percentage')

affair_yrs_married = pd.crosstab(data.children, data.affair.astype(bool))
affair_yrs_married.div(affair_yrs_married.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Affair Percentage by Children')
plt.xlabel('Children')
plt.ylabel('Percentage')

# @title Model Training
data.columns

data.sample(5)

y, X = dmatrices('affair ~ rate_marriage + age + yrs_married + children + religious + educ + C(occupation) + C(occupation_husb)', data, return_type="dataframe")
print(X.columns)
print(y)

X.head()

# fix column names of X
X = X.rename(columns = {'C(occupation)[T.2.0]':'occ_2',
                        'C(occupation)[T.3.0]':'occ_3',
                        'C(occupation)[T.4.0]':'occ_4',
                        'C(occupation)[T.5.0]':'occ_5',
                        'C(occupation)[T.6.0]':'occ_6',
                        'C(occupation_husb)[T.2.0]':'occ_husb_2',
                        'C(occupation_husb)[T.3.0]':'occ_husb_3',
                        'C(occupation_husb)[T.4.0]':'occ_husb_4',
                        'C(occupation_husb)[T.5.0]':'occ_husb_5',
                        'C(occupation_husb)[T.6.0]':'occ_husb_6'})

"""
df=pd.concat([X,y],axis=1)
df.isnull().values.any()
print(df[df["affair"]==0].shape)
print(df[df["affair"]==1].shape)
!pip install imbalanced-learn
from imblearn.over_sampling import SMOTE
oversample = SMOTE()
X,y=oversample.fit_resample(X,y)Q
print(df[df["affair"]==0].shape)
print(df[df["affair"]==1].shape)
"""



X.head()

y = np.ravel(y)

model = LogisticRegression()
model = model.fit(X, y)
# check the accuracy on the training set
model.score(X, y)

y.mean()

importance = model.coef_[0]
feature_names = list(X.columns)
# summarize feature importance
for i,v in enumerate(importance):
    print('%s, Score: %.4f' % (feature_names[i], v))

importance = model.coef_[0]
# Get the feature names
feature_names = list(X.columns)
# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, importance)
plt.xlabel("Importance")
plt.title("Feature Importance")
plt.show()

#@title Model Evaluation

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,shuffle = True)
print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)

X_test.head(1)

LR = LogisticRegression()
LR.fit(X_train, y_train)

pred = LR.predict(X_test)
print(pred)

probs = LR.predict_proba(X_test)
print(probs)

print(metrics.accuracy_score(y_test, pred))
print(metrics.roc_auc_score(y_test, probs[:, 1]))

print(metrics.confusion_matrix(y_test, pred))

print(metrics.classification_report(y_test, pred))

#@title Calculating scores using CVScore

from sklearn.model_selection import cross_val_score
scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10)
print(scores)
print("\n Mean Score", round((scores.mean())*100, 3))





#@title Predictions

X.sample(4)

X.columns

"""The dataset contains 6366 observations of 9 variables:


*   **rate_marriage**: woman's rating of her marriage (1 = very poor, 5 = very good)
*   **age**: woman's age
*   **yrs_married**: number of years married
*   **children**: number of children
*   **religious**: woman's rating of how religious she is (1 = not religious, 4 = strongly religious)
*   **educ**: level of education (9 = grade school, 12 = high school, 14 = some college, 16 = college graduate, 17 = some graduate school, 20 = advanced degree)
*   **occupation**: woman's occupation (1 = student, 2 = farming/semi-skilled/unskilled, 3 = "white collar", 4 = teacher/nurse/writer/technician/skilled, 5 = managerial/business, 6 = professional with advanced degree)
*   **occupation_husb**: husband's occupation (same coding as above)
*   **affairs**: time spent in extra-marital affairs
"""

model.predict_proba(np.array([[1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 3, 25, 3, 1, 4,
                              16]]))

# saving our model using pikle
import pickle
pickle.dump((data.to_dict()),open("affair.pkl",'wb'))
pickle.dump(LR,open('logistic_model.pkl','wb'))

